---
title: Optimize Google Cloud BigQuery and Control Cost
publishedAt: Apr 13, 2023
description: I unknowingly blew $3,000 in 6 hours on one query in Google Cloud BigQuery. Here's why and how to optimize the cost.
cover: /optimized/articles/optimize-google-cloud-bigquery-and-control-cost/hero.webp
category: Cloud
coverWidth: '1400'
coverHeight: '700'
---

## In this article

-

Let's go.

---

## How did It Happen?

I was developing a script to prepare data samples for customers that reached out for consultations. The samples have 100 rows in each file and they are partitioned by 55 locales. My query looks like this

```sql
SELECT *
FROM `project.dataset.table`
WHERE
  timestamp BETWEEN TIMESTAMP("2022-12-01") AND TIMESTAMP("2023-02-28")
  AND locale = "US"
LIMIT 100;
```

The data was stored in "europe-west-4" and the [pricing for querying is $6 per TB][2]. So by running the script, I processed

- 500 TB of data in total
- 3 TB of data per country on average
- $54 per data sample file on average

Very expensive.

## The Script that Costed $3,000

The script was written in [JavaScript modules][3].

```js:bq-samples.mjs
import { BigQuery } from "@google-cloud/bigquery";
import { Parser } from "@json2csv/plainjs";
import makeDir from "make-dir";
import { write } from "./write.mjs";
import { locales } from "./locales.mjs";
import { perf } from "./performance.mjs";

const q = (locale, start, end, limit) => `SELECT *
  FROM \`project.dataset.table\`
  WHERE
    timestamp BETWEEN TIMESTAMP("2022-12-01") AND TIMESTAMP("2023-02-28")
    AND locale = "${locale}"
  LIMIT ${limit}`

async function main() {
  const timer = perf()

  const dir = await makeDir('samples')
  const bigquery = new BigQuery()
  const csvParser = new Parser({})

  try {
    const jobs = locales.map((locale) => async () => {
      // get query result from BigQuery
      const [job] = await bigquery.createQueryJob({
        query: q(locale, "2022-12-01", "2023-02-28", 100),
      })
      const [rows] = await job.getQueryResults()

      // parse rows into csv format
      const csv = parse(csvParser, rows)

      // write data into csv files and store in the file system
      await write(csv, dir, locale, "2022-12-01", "2023-02-28", 100)
    })

    await Promise.all(jobs.map((job) => job()))

    console.log(`‚ú® Done in ${timer.stop()} seconds.`)
  } catch (error) {
    console.error('‚ùå Failed to create sample file', error)
  }
}

await main()
```

The script generates one sample file in the [CSV][4] format per locale. The process is straightforward:

- Querying the BigQuery table with a local, start date, end date, and limit.
- Parsing the query result in CSV.
- Writing the CSV into the file system.
- Repeating the process for all locales.

## What's The Problem?

<img
  src="/optimized/articles/optimize-google-cloud-bigquery-and-control-cost/hero.webp"
  alt="image from the template"
  width="100%"
  className="rounded centered"
  loading="lazy"
/>

## Final Thoughts

## References

- [A Look at Dremel][10] - Peter Goldsborough
- [BigQuery][1] - Google Cloud
- [BigQuery explained: An overview of BigQuery's architecture][9] - Google Cloud
- [Colossus under the hood: a peek into Google‚Äôs scalable storage system][7] - Google Cloud
- [Comma-separated values][4] - Wikipedia
- [JavaScript modules][3] - MDN
- [Jupiter evolving: Reflecting on Google‚Äôs data center network transformation][8] - Google Cloud
- [Method: jobs.query][13] - Google Cloud
- [Using cached query results][5] - Google Cloud
- [Writing query results][11] - Google Cloud

[1]: https://cloud.google.com/bigquery
[2]: https://cloud.google.com/bigquery/pricing
[3]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Modules
[4]: https://en.wikipedia.org/wiki/Comma-separated_values
[5]: https://cloud.google.com/bigquery/docs/cached-results#bq
[7]: https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system
[8]: https://cloud.google.com/blog/topics/systems/the-evolution-of-googles-jupiter-data-center-network
[9]: https://cloud.google.com/blog/products/data-analytics/new-blog-series-bigquery-explained-overview
[10]: http://www.goldsborough.me/distributed-systems/2019/05/18/21-09-00-a_look_at_dremel/
[11]: https://cloud.google.com/bigquery/docs/writing-results#node.js
[13]: https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/query#QueryRequest

---

Here you have it! Thanks for reading through üôå
If you find this article useful, please share it to help more people in their engineering journey.

üê¶ Feel free to connect with me on [twitter](https://twitter.com/dawchihliou)!

‚è≠Ô∏è Ready for the next article? üëâ [How to Run A Tech Community in Your Company: An Ex-Principal Engineer‚Äôs Guide](/articles/how-to-run-a-tech-community-in-your-company)

Happy coding!
